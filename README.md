ARLookbook
==========
AY-AR (COMPLETE-ish)
Apart from some debugging and some extra features we still have to add, we’re all done with our final project, AY-AR! The idea starting out was to have a Lookbook that would be in a store and allow you either to mix-and-match, view 3d models of items of clothing, view 2D images of clothing items along side a model wearing those items and try the clothes on. We used Augmented Reality and were able to complete it through the great help of Professor Kapp!
Our project is divided into a few parts.
2D aka Look-through
The 2D portion of the lookbook aka the “Look-Through” features a model in clothes from Madewell on the left pages and on the right pages, a fiducial marker that displays an array of the items as images. We use color detection to determine whether the user’s hand or an object is over the arrow and then increase or decrease the index of the array accordingly. Apart from that, the left page features a QR code that links to the items. We have 5 looks in the final lookbook.
3D aka Pop-through
The 3D portion of the lookbook aka the “Pop-Through” features a fiducial marker that represents a form of apparel i.e. a dress, skirt, shoe or bag. Each page differs in terms of the number of items you can loop through. The bag page features 2, the skirt page features one 3D model, the dress, 3 and the shoe 2. I wanted to have a set number for each and I originally started out thinking of having 5 for each but the dearth of good, free 3D models forced me to do otherwise (as well as the slowness of the program even with just that number). The 3D models were difficult to deal with – first to obtain, then to convert, and then to get materials and textures and finally to calculate appropriate scales and rotations for each and every one since they all started out with different locations and orientations. The 3D pages feature arrow buttons like with the 2D portion. It uses integer values assigned to different models. The Color Buttons use Booleans (color1Detected etc.) to determine what has been touched. Both the color buttons and the arrow controls use color detection.
Mix-and-match
The mix-and-match section uses fiducial makers and proximity sensing with augmented reality. The idea would be items in the store (or special items) have a fiducial marker that you can throw on a flat surface and view 3D models of items together. The program would then determine what type of apparel the marker represents and then determine the logical way to show these items together. So for example, a marker that represents a dress would not have a 3D model that displayed below a marker that represented a shoe (it would be the dress and the shoe below it). This part still held the same problem as he 3D in terms of working with the 3D models.
AR Fitting room
This was originally what the whole final project was supposed to be on. We were going to use the Kinect for background removal and to get the outline of the user. But unfortunately, the Kinect library for Processing is currently under construction. This would have fit in with the 3D Models, which we also planned to do, but again due to the dearth of good, free ones, we didn’t. The AR Fitting room was the last part of the project we implemented (without debugging and the extra features). It featured an interface with two arrows (one left and one right) as well as a camera. We used the Motion Detector class Professor Kapp wrote to determine if there was motion around or on the buttons. This would trigger an action. For the arrows, it would change the index of the array and then display the next item of clothing (which was strictly shirts from the 2D lookbook because they looked the most realistic). We photo-shopped the pieces and then just adjusted the size to fit. The camera starts up a timer and then saves a picture of the user wearing the item.
